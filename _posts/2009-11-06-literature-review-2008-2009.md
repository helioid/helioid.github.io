---
layout: post
title: Literature Review 2008 - 2009
categories:
- General
tags:
- learning
- models
- refinement
status: publish
type: post
published: true
meta:
  _edit_last: '1'
---
<p>
  <font size=2><b>Machine Learning</b></font>
</p>
<p>
  <font size=2><a href=http://www.cc.gatech.edu/%7Elebanon/ id=jrbv title="G. Lebanon">G. Lebanon</a>, <a href=http://web.ics.purdue.edu/%7Eymao/>Y. Mao</a>, and <a href=http://web.ics.purdue.edu/%7Ejvdillon/>J. Dillon</a>. <a href=http://jmlr.csail.mit.edu/papers/v8/lebanon07a.html>The Locally Weighted Bag of Words Framework for Document Representation</a>. <i>Journal of Machine Learning Research</i> <b>8</b> (Oct):2405-2441, 2007.</font>
</p>
<p>
  <font size=2>Summary: the lowbow framework captures topic trends in a document by applying a "local smoothing kernel to smooth the original word sequence temporally." (3)&nbsp; This provides a metric for the distance between two documents: the integrand of the path distance over the documents (11) or a diffusion kernel (11).&nbsp; Dynamic time warping can be used to reduce the effects of word order (25) although this proved negligibly helpful in experiments, perhaps because of a homogenous corpus or the robustness of the algorithm (26).&nbsp; Linear differential operators can be applied to the curve to find topic trends (tangent vector field) and document variability (integrand of the curvature tensor norm) (12).</font>
</p>
<p>
  <font size=2><br>
  <a href=http://www.cs.princeton.edu/%7Eblei/ id=beod title=D. Blei>D. Blei</a> and J. Lafferty. <b>A correlated topic model of <i>Science</i></b>. Annals of Applied Statistics. 1:1 17-35. (<a href=http://www.cs.princeton.edu/%7Eblei/papers/BleiLafferty2007.pdf>PDF</a>) (<a href=http://www.cs.princeton.edu/%7Eblei/papers/BleiLafferty2006.pdf>shorter version</a> from NIPS 18) (<a href=http://www.cs.princeton.edu/%7Eblei/ctm-c/>code</a>)(<a href=http://www.cs.cmu.edu/%7Elemur/science/>browser</a>) April 2007.<br>
  </font>
</p>
<p>
  <font size=2>Summary: The CTM is able to capture correlation between topics (unlike LDA).&nbsp; Applied algorithm to the Science journal corpus to generate a topic model.&nbsp; Used a graphing algorithm to describe connections between latent topics.&nbsp; Associated each document with a latent vector of topics, which allows a user to browse documents by topic(s) they are related to.</font>
</p>
<p class=style2 style=FONT-WEIGHT:normal>
  <a href=http://www.cs.umass.edu/%7Egiridhar/ id=vh8m title="G. Kumaran"><br>
  </a>
</p>
<p class=style2 style=FONT-WEIGHT:normal>
  <font size=2><a href=http://www.cs.umass.edu/%7Egiridhar/ id=y3u3 title="G. Kumaran">G. Kumaran</a> and <a href=http://ciir.cs.umass.edu/%7Eallan/ id=zl5i title="J. Allan">J. Allan</a>.</font><font size=2> <a href=http://ciir-publications.cs.umass.edu/getpdf.php?id=806 id=vvln title="Effective and Efﬁcient User Interaction for Long Queries">Effective and Efficient User Interaction for Long Queries</a>, Proceedings of the 31st Annual International ACM SIGIR Conference, pp. 11-18. July 2008.<br>
  </font>
</p>
<p>
  <font size=2>Summary: Selective interactive reduction and expansion (SIRE) of queries is used so users can interactively narrow returned search results.</font>
</p>
<font size=2 style=FONT-FAMILY:Verdana><br>
<a href=http://www.cs.umass.edu/%7Eyixing/ id=rlkr title="X. Yi">X. Yi</a> and J. Allan. </font><font size=2 style=FONT-FAMILY:Verdana><a href=http://ciir-publications.cs.umass.edu/getpdf.php?id=807 id=oq1_ title="Evaluating Topic Models for Information Retrieval">Evaluating Topic Models for Information Retrieval</a></font><font class=style3 face="Times New Roman" size=2 style=FONT-FAMILY:Verdana>, to appear in the Proceedings of ACM 17th Conference on Information and Knowledge Management, Napa Valley, CA, October 26-30, 2008.<br>
Summary: From the abstract, "</font><font size=2>(1) topic models are effective for document smoothing; (2) more elaborate topic models that capture topic dependencies provide no additional gains; (3) smoothing documents by using their similar documents is as effective as smoothing them by using topic models; (4) topics discovered on the whole corpus are too coarse-grained to be useful for query expansion. Experiments to measure topic models' ability to predict held-out likelihood confirm past results on small corpora, but suggest that simple approaches to topic model are better for large corpora."<br>
<br>
<a href=http://www.cs.umass.edu/%7Emimno/ id=fkzr title="D. Mimno">D. Mimno</a>, <a href=http://www.cs.umass.edu/%7Emccallum/ id=wmun title="A. McCallum">A. McCallum</a>. <a href=http://www.cs.umass.edu/%7Emimno/papers/dmr-uai.pdf id=l62g title="Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression">Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression</a>, To appear in UAI, 2008.<br>
Summary: The authors propose Dirichlet-multinominal regression (DMR) which generates the prior distribution over topics specific to each document and based upon its observed features.&nbsp; In results predict topics from author documents and also define a prior over topics and use for author prediction of documents.&nbsp; Impressive performances in comparison with LDA and AT models (5).&nbsp; Sampling phase in DMR no more complex (slow) than LDA (7).&nbsp; Could potentially be extended to a hierarchical model (7).<br>
<br>
<a href=http://www.cs.umass.edu/%7Emccallum/papers/art-jair07.pdf>Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email</a>. Andrew McCallum, Xuerui Wang and Andres Corrada-Emmanuel. Journal of Artificial Intelligence Research (<b>JAIR</b>), 2007.<br>
Summary: Presents Author-Recipient-Topic (ART) model in which each topic is a multinomial distribution over words and each author, recipient pair is a multinomial distribution over topics. People can be clustered based upon topics to elicit roles independent of the network of people they are connected to (2). Role-Author-Recipient-Topic (RART) generates explicit roles and conditions the topics based upon them (3).&nbsp; Potential extension to incorporate temporal information into the model (7).&nbsp; Can generate topic relevance measures for an author relative to the roles they occupy (20).<br>
<br>
G. Xue, et al. <a href=http://labs.rightnow.com/colloquium/papers/link_analysis_for_search.pdf id=itb0 title="Implicit Link Analysis for Small Web Search">Implicit Link Analysis for Small Web Search</a>. 2003.<br>
Summary: Differentiates <i>navigational</i> links, <i>recommendation</i> links, and <i>implicit</i> <i>recommendation</i> links that are based on usage patterns.&nbsp; Implicit links correspond to log entries, an implicit path is a path through implicit links, an explicit path is a path from one implicit link to another with explicit links in between.&nbsp; The weight of an edge in the implicit link graph is it's normalized support (3).&nbsp; Experimentally the higher the support the more precise the implicit link and an implicit link is to be a recommendation link than an explicit link (4).&nbsp; In experiments using pages ranked by people as the ideal implicit PageRank showed considerable improvements over full text, explicit PageRank, DirectHit, and modified-HITS algorithms (5).&nbsp; Shows that in small webs links do not necessarily meet the requirement of being recommendations that is needed for PageRank to function effectively.<br>
<br>
C. Wang, D. Blei, and D. Heckerman. <b>Continuous time dynamic topic models.</b> In <i>Uncertainty in Artificial Intelligence (UAI)</i>, 2008. <a href=http://www.cs.princeton.edu/%7Eblei/papers/WangBleiHeckerman2008.pdf>(PDF)</a>.<br>
<br>
X. Wang, A. McCallum. <a href=http://www.cs.umass.edu/%7Emccallum/papers/tot-kdd06.pdf id=wzm_ title="Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends">Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends</a>. <i>KDD</i>, August 2006.<br>
</font><font size=2><br>
A. Grubber, M. Rosen-Zvi, Y. Weiss. <a class=l href=http://www.cs.huji.ac.il/%7Eyweiss/uai08gruber.pdf>Latent <i>Topic Models</i> for Hypertext</a></font><font size=2>. 2008.<br>
<br>
<a class=l href=http://www.cs.umass.edu/%7Emccallum/papers/crf-tutorial.pdf>An Introduction to Conditional Random Fields for Relational Learning</a></font><br>
<font size=2><b><br>
Medical</b><br>
<br>
<a href=http://pages.cs.wisc.edu/%7Egangluo/ id=uxc- title="G. Luo">G. Luo</a>, C. Tang, H. Yang, and X. Wei. <i>MedSearch: A Specialized Search Engine for Medical Information Retrieval. </i> [<a href=http://www.cs.wisc.edu/%7Egangluo/medsearch_cikm08.pdf>pdf</a>] Proc. 2008 ACM Conf. on Information and Knowledge Management (CIKM'08), industry session, Napa Valley, CA, Oct. 2008, pp. ?-?.<br>
Summary: Medical search different because users search more exploratory, search through related information.&nbsp; Queries are generally longer and Google has a 32 word limit, other engines generally have length limits.&nbsp; MedSearch drops unimportant terms, returns diverse search results, and suggests medical phrases relevant to query.&nbsp; Uses an establish hierarchical ontology of medical terms (MeSH) to identify and rank medical phrases in the returned top web pages (3).<br>
<br>
G. Luo, C. Tang. <i>On Iterative Intelligent Medical Search. </i> [<a href=http://www.cs.wisc.edu/%7Egangluo/imedII_sigir08.pdf>pdf</a>] Proc. 2008 Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR'08), Singapore, July 2008, pp. 3-10.<br>
Summary: First, relevant symptoms and signs are automatically suggested based on the searcher's description of his situation. Second, instead of taking for granted the searcher's answers to the questions, iMed ranks and recommends alternative answers according to their likelihoods of being the correct answers. Third, related MeSH medical phrases are suggested to help the searcher refine his situation description.<br>
<br>
<b>Enterprise<br>
</b><br>
Hawking, D. (2004). <a href=http://crpit.com/confpapers/CRPITV27Hawking1.pdf>Challenges in Enterprise Search</a>. In Proc. Fifteenth Australasian Database Conference (ADC2004), Dunedin, New Zealand. CRPIT, <b>27</b>. Schewe, K.-D. and Williams, H. E., Eds. ACS. 15-24.<br>
Summary: Review of enterprise search problems including "3.5 Estimating importance of non-web documents" (6) to which topic based probability linking seems a good solution.&nbsp; Exploiting context is also mentioned (3.6, 6).&nbsp; The proposal to convert documents to HTML so that they can be analyzed as normal is criticized for lack of link information documents would have.<br>
<br>
<b>Parsing</b><br>
<br>
<a href=http://www.cs.umass.edu/%7Emccallum/papers/dp3ws08.pdf>Bayesian Modeling of Dependency Trees Using Hierarchical Pitman-Yor Priors</a>. Hanna Wallach, Charles Sutton, Andrew McCallum. In International Conference on Machine Learning, Workshop on Prior Knowledge for Text and Language Processing. (<b>ICML WS</b>), 2008.<br>
Summary: Describe two hierarchical models for Bayesian dependency trees and use them to parse sentences.&nbsp; Use latent variables to cluster parent-child dependencies resulting in improved parse accuracy.<br>
<br>
<b>Bibliometrics<br>
</b><br>
B. Zhang, et al.&nbsp; <a href=http://portal.acm.org/citation.cfm?id=1076181#references id=kz1j title="Intelligent Fusion of Structural and Citation-Based Evidence for Text Classification">Intelligent Fusion of Structural and Citation-Based Evidence for Text Classification</a>. <span class=mediumb-text>Annual ACM Conference on Research and Development in Information Retrieval</span>.&nbsp; 2005.<br>
Summary: Use GP to combine various bibliometrics with +, *, /, sqrt (why no -?).&nbsp; Found GP trees perform better than individual metrics and slightly better than SVM, although SVM greatly outperform there method on some categories.&nbsp; Is there a way to choose the better of SVM and GP per category?&nbsp; Paper is poorly written.<br>
<br>
<b>User Interaction Design<br>
</b>M. Schmettow. <a href=http://hillside.net/europlop/europlop2006/workshops/C6.pdf id=u_s3 title="User Interaction Design Patterns for Information Retrieval  Systems">User Interaction Design Patterns for Information Retrieval&nbsp; Systems</a>.<br>
<br>
<b>ML Software &amp; Toolkits</b><br>
<br>
<a href=http://mallet.cs.umass.edu/ id=ifi7 title=MALLET>MALLET</a> is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.<br>
</font>
<p>
  <font size=2><a href=http://www.cs.umass.edu/%7Eronb/ id=ay8d title="R. Bekkerman">R. Bekkerman</a>. <a href=http://www.cs.umass.edu/%7Eronb/papers/mdc-doc.pdf id=r9q- title="MDC Documentation">MDC Documentation</a><br>
  </font>
</p>
<font size=2>Summary: The <a href=http://www.cs.umass.edu/%7Eronb/mdc.html id=ls3_ title="MDC Toolkit">MDC Toolkit</a> takes a contingency table and parameters file and returns clusters.<br>
